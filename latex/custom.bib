@misc{abbasLossPlasticityContinual2023,
  title = {Loss of {{Plasticity}} in {{Continual Deep Reinforcement Learning}}},
  author = {Abbas, Zaheer and Zhao, Rosie and Modayil, Joseph and White, Adam and Machado, Marlos C.},
  year = {2023},
  month = mar,
  number = {arXiv:2303.07507},
  eprint = {2303.07507},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-19},
  abstract = {The ability to learn continually is essential in a complex and changing world. In this paper, we characterize the behavior of canonical value-based deep reinforcement learning (RL) approaches under varying degrees of non-stationarity. In particular, we demonstrate that deep RL agents lose their ability to learn good policies when they cycle through a sequence of Atari 2600 games. This phenomenon is alluded to in prior work under various guises -- e.g., loss of plasticity, implicit under-parameterization, primacy bias, and capacity loss. We investigate this phenomenon closely at scale and analyze how the weights, gradients, and activations change over time in several experiments with varying dimensions (e.g., similarity between games, number of games, number of frames per game), with some experiments spanning 50 days and 2 billion environment interactions. Our analysis shows that the activation footprint of the network becomes sparser, contributing to the diminishing gradients. We investigate a remarkably simple mitigation strategy -- Concatenated ReLUs (CReLUs) activation function -- and demonstrate its effectiveness in facilitating continual learning in a changing environment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,plasticity},
  file = {/Users/darshanpatil/Zotero/storage/RNVWGILR/Abbas et al. - 2023 - Loss of Plasticity in Continual Deep Reinforcement.pdf;/Users/darshanpatil/Zotero/storage/JQKHZKZ7/2303.html}
}

@misc{achilleCriticalLearningPeriods2019,
  title = {Critical {{Learning Periods}} in {{Deep Neural Networks}}},
  author = {Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
  year = {2019},
  month = feb,
  number = {arXiv:1711.08856},
  eprint = {1711.08856},
  primaryclass = {cs, q-bio, stat},
  publisher = {arXiv},
  urldate = {2023-08-28},
  abstract = {Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training. To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training. Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of "Information Plasticity". Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/PHJD4WNI/Achille et al. - 2019 - Critical Learning Periods in Deep Neural Networks.pdf;/Users/darshanpatil/Zotero/storage/978AVD7F/1711.html}
}

@misc{ashWarmStartingNeuralNetwork2020,
  title = {On {{Warm-Starting Neural Network Training}}},
  author = {Ash, Jordan T. and Adams, Ryan P.},
  year = {2020},
  month = dec,
  number = {arXiv:1910.08475},
  eprint = {1910.08475},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-07-12},
  abstract = {In many real-world deployments of machine learning systems, data arrive piecemeal. These learning scenarios may be passive, where data arrive incrementally due to structural properties of the problem (e.g., daily financial data) or active, where samples are selected according to a measure of their quality (e.g., experimental design). In both of these cases, we are building a sequence of models that incorporate an increasing amount of data. We would like each of these models in the sequence to be performant and take advantage of all the data that are available to that point. Conventional intuition suggests that when solving a sequence of related optimization problems of this form, it should be possible to initialize using the solution of the previous iterate -- to "warm start" the optimization rather than initialize from scratch -- and see reductions in wall-clock time. However, in practice this warm-starting seems to yield poorer generalization performance than models that have fresh random initializations, even though the final training losses are similar. While it appears that some hyperparameter settings allow a practitioner to close this generalization gap, they seem to only do so in regimes that damage the wall-clock gains of the warm start. Nevertheless, it is highly desirable to be able to warm-start neural network training, as it would dramatically reduce the resource usage associated with the construction of performant deep learning systems. In this work, we take a closer look at this empirical phenomenon and try to understand when and how it occurs. We also provide a surprisingly simple trick that overcomes this pathology in several important situations, and present experiments that elucidate some of its properties.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,plasticity,Statistics - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/YRMNUVGU/Ash and Adams - 2020 - On Warm-Starting Neural Network Training.pdf;/Users/darshanpatil/Zotero/storage/532JPW4K/1910.html}
}

@misc{berariuStudyPlasticityNeural2021,
  title = {A Study on the Plasticity of Neural Networks},
  author = {Berariu, Tudor and Czarnecki, Wojciech and De, Soham and Bornschein, Jorg and Smith, Samuel and Pascanu, Razvan and Clopath, Claudia},
  year = {2021},
  month = may,
  number = {arXiv:2106.00042},
  eprint = {2106.00042},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-12},
  abstract = {One aim shared by multiple settings, such as continual learning or transfer learning, is to leverage previously acquired knowledge to converge faster on the current task. Usually this is done through fine-tuning, where an implicit assumption is that the network maintains its plasticity, meaning that the performance it can reach on any given task is not affected negatively by previously seen tasks. It has been observed recently that a pretrained model on data from the same distribution as the one it is fine-tuned on might not reach the same generalisation as a freshly initialised one. We build and extend this observation, providing a hypothesis for the mechanics behind it. We discuss the implication of losing plasticity for continual learning which heavily relies on optimising pretrained models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,plasticity},
  file = {/Users/darshanpatil/Zotero/storage/7ZF6L9VT/Berariu et al. - 2021 - A study on the plasticity of neural networks.pdf;/Users/darshanpatil/Zotero/storage/5I8X947W/2106.html}
}

@misc{bjorckHighVarianceUnavoidable2022,
  title = {Is {{High Variance Unavoidable}} in {{RL}}? {{A Case Study}} in {{Continuous Control}}},
  shorttitle = {Is {{High Variance Unavoidable}} in {{RL}}?},
  author = {Bjorck, Johan and Gomes, Carla P. and Weinberger, Kilian Q.},
  year = {2022},
  month = feb,
  number = {arXiv:2110.11222},
  eprint = {2110.11222},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-16},
  abstract = {Reinforcement learning (RL) experiments have notoriously high variance, and minor details can have disproportionately large effects on measured outcomes. This is problematic for creating reproducible research and also serves as an obstacle for real-world applications, where safety and predictability are paramount. In this paper, we investigate causes for this perceived instability. To allow for an in-depth analysis, we focus on a specifically popular setup with high variance -- continuous control from pixels with an actor-critic agent. In this setting, we demonstrate that variance mostly arises early in training as a result of poor "outlier" runs, but that weight initialization and initial exploration are not to blame. We show that one cause for early variance is numerical instability which leads to saturating nonlinearities. We investigate several fixes to this issue and find that one particular method is surprisingly effective and simple -- normalizing penultimate features. Addressing the learning instability allows for larger learning rates, and significantly decreases the variance of outcomes. This demonstrates that the perceived variance in RL is not necessarily inherent to the problem definition and may be addressed through simple architectural modifications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,plasticity},
  file = {/Users/darshanpatil/Zotero/storage/CBLD9MYZ/Bjorck et al. - 2022 - Is High Variance Unavoidable in RL A Case Study i.pdf;/Users/darshanpatil/Zotero/storage/F4CA3J9Z/2110.html}
}

@misc{chenStochasticCollapseHow2023,
  title = {Stochastic {{Collapse}}: {{How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks}}},
  shorttitle = {Stochastic {{Collapse}}},
  author = {Chen, Feng and Kunin, Daniel and Yamamura, Atsushi and Ganguli, Surya},
  year = {2023},
  month = jun,
  number = {arXiv:2306.04251},
  eprint = {2306.04251},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-06-23},
  abstract = {In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss. We observe empirically the existence of attractive invariant sets in trained deep neural networks, implying that SGD dynamics often collapses to simple subnetworks with either vanishing or redundant neurons. We further demonstrate how this simplifying process of stochastic collapse benefits generalization in a linear teacher-student framework. Finally, through this analysis, we mechanistically explain why early training with large learning rates for extended periods benefits subsequent generalization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,plasticity,Statistics - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/VGBVPPYD/Chen et al. - 2023 - Stochastic Collapse How Gradient Noise Attracts S.pdf;/Users/darshanpatil/Zotero/storage/NMX4XGU9/2306.html}
}

@misc{cohenGradientDescentNeural2022,
  title = {Gradient {{Descent}} on {{Neural Networks Typically Occurs}} at the {{Edge}} of {{Stability}}},
  author = {Cohen, Jeremy M. and Kaur, Simran and Li, Yuanzhi and Kolter, J. Zico and Talwalkar, Ameet},
  year = {2022},
  month = nov,
  number = {arXiv:2103.00065},
  eprint = {2103.00065},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-06-27},
  abstract = {We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the numerical value \$2 / {\textbackslash}text\{(step size)\}\$, and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability. Code is available at https://github.com/locuslab/edge-of-stability.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,plasticity,Statistics - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/WFK4DJYQ/Cohen et al. - 2022 - Gradient Descent on Neural Networks Typically Occu.pdf;/Users/darshanpatil/Zotero/storage/7LNDAK95/2103.html}
}

@misc{dohareContinualBackpropStochastic2022,
  title = {Continual {{Backprop}}: {{Stochastic Gradient Descent}} with {{Persistent Randomness}}},
  shorttitle = {Continual {{Backprop}}},
  author = {Dohare, Shibhansh and Sutton, Richard S. and Mahmood, A. Rupam},
  year = {2022},
  month = may,
  number = {arXiv:2108.06325},
  eprint = {2108.06325},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-08-01},
  abstract = {The Backprop algorithm for learning in neural networks utilizes two mechanisms: first, stochastic gradient descent and second, initialization with small random weights, where the latter is essential to the effectiveness of the former. We show that in continual learning setups, Backprop performs well initially, but over time its performance degrades. Stochastic gradient descent alone is insufficient to learn continually; the initial randomness enables only initial learning but not continual learning. To the best of our knowledge, ours is the first result showing this degradation in Backprop's ability to learn. To address this degradation in Backprop's plasticity, we propose an algorithm that continually injects random features alongside gradient descent using a new generate-and-test process. We call this the {\textbackslash}textit\{Continual Backprop\} algorithm. We show that, unlike Backprop, Continual Backprop is able to continually adapt in both supervised and reinforcement learning (RL) problems. Continual Backprop has the same computational complexity as Backprop and can be seen as a natural extension of Backprop for continual learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,plasticity},
  file = {/Users/darshanpatil/Zotero/storage/437WXBTY/Dohare et al. - 2022 - Continual Backprop Stochastic Gradient Descent wi.pdf;/Users/darshanpatil/Zotero/storage/7KWHM56B/2108.html}
}

@article{doroSAMPLEEFFICIENTREINFORCEMENTLEARNING2023,
  title = {{{SAMPLE-EFFICIENT REINFORCEMENT LEARNING BY BREAKING THE REPLAY RATIO BARRIER}}},
  author = {D'Oro, Pierluca and Schwarzer, Max and Nikishin, Evgenii and Bacon, Pierre-Luc and Bellemare, Marc G and Courville, Aaron},
  year = {2023},
  abstract = {Increasing the replay ratio, the number of updates of an agent's parameters per environment interaction, is an appealing strategy for improving the sample efficiency of deep reinforcement learning algorithms. In this work, we show that fully or partially resetting the parameters of deep reinforcement learning agents causes better replay ratio scaling capabilities to emerge. We push the limits of the sample efficiency of carefully-modified algorithms by training them using an order of magnitude more updates than usual, significantly improving their performance in the Atari 100k and DeepMind Control Suite benchmarks. We then provide an analysis of the design choices required for favorable replay ratio scaling to be possible and discuss inherent limits and tradeoffs.},
  langid = {english},
  keywords = {plasticity},
  file = {/Users/darshanpatil/Zotero/storage/PKQEH6TS/D’Oro et al. - 2023 - SAMPLE-EFFICIENT REINFORCEMENT LEARNING BY BREAKIN.pdf}
}

@misc{elsayedUtilitybasedPerturbedGradient2023,
  title = {Utility-Based {{Perturbed Gradient Descent}}: {{An Optimizer}} for {{Continual Learning}}},
  shorttitle = {Utility-Based {{Perturbed Gradient Descent}}},
  author = {Elsayed, Mohamed and Mahmood, A. Rupam},
  year = {2023},
  month = apr,
  number = {arXiv:2302.03281},
  eprint = {2302.03281},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.03281},
  urldate = {2023-06-16},
  abstract = {Modern representation learning methods often struggle to adapt quickly under non-stationarity because they suffer from catastrophic forgetting and decaying plasticity. Such problems prevent learners from fast adaptation since they may forget useful features or have difficulty learning new ones. Hence, these methods are rendered ineffective for continual learning. This paper proposes Utility-based Perturbed Gradient Descent (UPGD), an online learning algorithm well-suited for continual learning agents. UPGD protects useful weights or features from forgetting and perturbs less useful ones based on their utilities. Our empirical results show that UPGD helps reduce forgetting and maintain plasticity, enabling modern representation learning methods to work effectively in continual learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,plasticity},
  file = {/Users/darshanpatil/Zotero/storage/TTPRCZUJ/Elsayed and Mahmood - 2023 - Utility-based Perturbed Gradient Descent An Optim.pdf;/Users/darshanpatil/Zotero/storage/JCCXPCSN/2302.html}
}

@misc{frankleLinearModeConnectivity2020,
  title = {Linear {{Mode Connectivity}} and the {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
  year = {2020},
  month = jul,
  number = {arXiv:1912.05671},
  eprint = {1912.05671},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.05671},
  urldate = {2023-06-27},
  abstract = {We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,plasticity,Statistics - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/74CGXDUP/Frankle et al. - 2020 - Linear Mode Connectivity and the Lottery Ticket Hy.pdf;/Users/darshanpatil/Zotero/storage/KQJPTMEJ/1912.html}
}

@misc{iglTransientNonStationarityGeneralisation2021,
  title = {Transient {{Non-Stationarity}} and {{Generalisation}} in {{Deep Reinforcement Learning}}},
  author = {Igl, Maximilian and Farquhar, Gregory and Luketina, Jelena and Boehmer, Wendelin and Whiteson, Shimon},
  year = {2021},
  month = sep,
  number = {arXiv:2006.05826},
  eprint = {2006.05826},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.05826},
  urldate = {2023-06-16},
  abstract = {Non-stationarity can arise in Reinforcement Learning (RL) even in stationary environments. For example, most RL algorithms collect new data throughout training, using a non-stationary behaviour policy. Due to the transience of this non-stationarity, it is often not explicitly addressed in deep RL and a single neural network is continually updated. However, we find evidence that neural networks exhibit a memory effect where these transient non-stationarities can permanently impact the latent representation and adversely affect generalisation performance. Consequently, to improve generalisation of deep RL agents, we propose Iterated Relearning (ITER). ITER augments standard RL training by repeated knowledge transfer of the current policy into a freshly initialised network, which thereby experiences less non-stationarity during training. Experimentally, we show that ITER improves performance on the challenging generalisation benchmarks ProcGen and Multiroom.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,plasticity,Statistics - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/YH634LPA/Igl et al. - 2021 - Transient Non-Stationarity and Generalisation in D.pdf;/Users/darshanpatil/Zotero/storage/KDLD6F8F/2006.html}
}

@misc{kumarMaintainingPlasticityContinual2023,
  title = {Maintaining {{Plasticity}} in {{Continual Learning}} via {{Regenerative Regularization}}},
  author = {Kumar, Saurabh and Marklund, Henrik and Van Roy, Benjamin},
  year = {2023},
  month = oct,
  number = {arXiv:2308.11958},
  eprint = {2308.11958},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.11958},
  urldate = {2023-10-20},
  abstract = {In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters should drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On problems representative of different types of nonstationarity in continual supervised learning, we demonstrate that L2 Init most consistently mitigates plasticity loss compared to previously proposed approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,plasticity},
  file = {/Users/darshanpatil/Zotero/storage/UCNW7LDD/Kumar et al. - 2023 - Maintaining Plasticity in Continual Learning via R.pdf;/Users/darshanpatil/Zotero/storage/DWXZUJNJ/2308.html}
}

@inproceedings{langeContinualEvaluationLifelong2022,
  title = {Continual Evaluation for Lifelong Learning: {{Identifying}} the Stability Gap},
  shorttitle = {Continual Evaluation for Lifelong Learning},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Lange, Matthias De and van de Ven, Gido M. and Tuytelaars, Tinne},
  year = {2022},
  month = sep,
  urldate = {2023-11-20},
  abstract = {Time-dependent data-generating distributions have proven to be difficult for gradient-based training of neural networks, as the greedy updates result in catastrophic forgetting of previously learned knowledge. Despite the progress in the field of continual learning to overcome this forgetting, we show that a set of common state-of-the-art methods still suffers from substantial forgetting upon starting to learn new tasks, except that this forgetting is temporary and followed by a phase of performance recovery. We refer to this intriguing but potentially problematic phenomenon as the stability gap. The stability gap had likely remained under the radar due to standard practice in the field of evaluating continual learning models only after each task. Instead, we establish a framework for continual evaluation that uses per-iteration evaluation and we define a new set of metrics to quantify worst-case performance. Empirically we show that experience replay, constraint-based replay, knowledge-distillation, and parameter regularization methods are all prone to the stability gap; and that the stability gap can be observed in class-, task-, and domain-incremental learning benchmarks. Additionally, a controlled experiment shows that the stability gap increases when tasks are more dissimilar. Finally, by disentangling gradients into plasticity and stability components, we propose a conceptual explanation for the stability gap.},
  langid = {english},
  file = {/Users/darshanpatil/Zotero/storage/XF7J5L3P/Lange et al. - 2022 - Continual evaluation for lifelong learning Identi.pdf}
}

@misc{lewandowskiDirectionsCurvatureExplanation2024,
  title = {Directions of {{Curvature}} as an {{Explanation}} for {{Loss}} of {{Plasticity}}},
  author = {Lewandowski, Alex and Tanaka, Haruto and Schuurmans, Dale and Machado, Marlos C.},
  year = {2024},
  month = feb,
  number = {arXiv:2312.00246},
  eprint = {2312.00246},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.00246},
  urldate = {2024-05-28},
  abstract = {Loss of plasticity is a phenomenon in which neural networks lose their ability to learn from new experience. Despite being empirically observed in several problem settings, little is understood about the mechanisms that lead to loss of plasticity. In this paper, we offer a consistent explanation for loss of plasticity: Neural networks lose directions of curvature during training and that loss of plasticity can be attributed to this reduction in curvature. To support such a claim, we provide a systematic investigation of loss of plasticity across continual learning tasks using MNIST, CIFAR-10 and ImageNet. Our findings illustrate that loss of curvature directions coincides with loss of plasticity, while also showing that previous explanations are insufficient to explain loss of plasticity in all settings. Lastly, we show that regularizers which mitigate loss of plasticity also preserve curvature, motivating a simple distributional regularizer that proves to be effective across the problem settings we considered.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/NIFUJRIY/Lewandowski et al. - 2024 - Directions of Curvature as an Explanation for Loss.pdf;/Users/darshanpatil/Zotero/storage/V85ZT6SC/2312.html}
}

@misc{lewkowyczLargeLearningRate2020,
  title = {The Large Learning Rate Phase of Deep Learning: The Catapult Mechanism},
  shorttitle = {The Large Learning Rate Phase of Deep Learning},
  author = {Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and {Sohl-Dickstein}, Jascha and {Gur-Ari}, Guy},
  year = {2020},
  month = mar,
  number = {arXiv:2003.02218},
  eprint = {2003.02218},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.02218},
  urldate = {2023-06-27},
  abstract = {The choice of initial learning rate can have a profound effect on the performance of deep networks. We present a class of neural networks with solvable training dynamics, and confirm their predictions empirically in practical deep learning settings. The networks exhibit sharply distinct behaviors at small and large learning rates. The two regimes are separated by a phase transition. In the small learning rate phase, training can be understood using the existing theory of infinitely wide neural networks. At large learning rates the model captures qualitatively distinct phenomena, including the convergence of gradient descent dynamics to flatter minima. One key prediction of our model is a narrow range of large, stable learning rates. We find good agreement between our model's predictions and training dynamics in realistic deep learning settings. Furthermore, we find that the optimal performance in such settings is often found in the large learning rate phase. We believe our results shed light on characteristics of models trained at different learning rates. In particular, they fill a gap between existing wide neural network theory, and the nonlinear, large learning rate, training dynamics relevant to practice.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,plasticity,Statistics - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/7XHPL2YK/Lewkowycz et al. - 2020 - The large learning rate phase of deep learning th.pdf;/Users/darshanpatil/Zotero/storage/CKUQE3ZV/2003.html}
}

@article{LossDecouplingContinual2023,
  title = {Loss {{Decoupling}} for {{Continual Learning}}},
  year = {2023},
  month = may,
  urldate = {2023-06-25},
  abstract = {Continual learning requires the model to learn multiple tasks in a sequential order. To perform continual learning, the model must possess the abilities to maintain performance on old tasks (stability) and adapt itself to learn new tasks (plasticity). Task-agnostic problem in continual learning is a challenging problem, in which task identities are not available in the inference stage and hence the model must learn to distinguish all the classes in all the tasks. In task-agnostic problem, the model needs to learn two new objectives for learning a new task, including distinguishing new classes from old classes and distinguishing between different new classes. For task-agnostic problem, replay-based methods are commonly used. These methods update the model with both saved old samples and new samples for continual learning. Most existing replay-based methods mix the two objectives in task-agnostic problem together, inhibiting the models from achieving a good trade-off between stability and plasticity. In this paper, we propose a simple yet effective method, called loss decoupling (LODE), for continual learning. LODE separates the two objectives for the new task by decoupling the loss of the new task. As a result, LODE can assign different weights for different objectives, which provides a way to obtain a better trade-off between stability and plasticity than those methods with coupled loss. Experiments show that LODE can outperform existing state-of-the-art replay-based methods on multiple continual learning datasets.},
  langid = {english},
  file = {/Users/darshanpatil/Zotero/storage/J5Z5DI95/2023 - Loss Decoupling for Continual Learning.pdf}
}

@misc{lyleDisentanglingCausesPlasticity2024,
  title = {Disentangling the {{Causes}} of {{Plasticity Loss}} in {{Neural Networks}}},
  author = {Lyle, Clare and Zheng, Zeyu and Khetarpal, Khimya and {van Hasselt}, Hado and Pascanu, Razvan and Martens, James and Dabney, Will},
  year = {2024},
  month = feb,
  number = {arXiv:2402.18762},
  eprint = {2402.18762},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-03},
  abstract = {Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly innocuous assumption: that the network is trained on a {\textbackslash}textit\{stationary\} data distribution. In settings where this assumption is violated, e.g.{\textbackslash} deep reinforcement learning, learning algorithms become unstable and brittle with respect to hyperparameters and even random seeds. One factor driving this instability is the loss of plasticity, meaning that updating the network's predictions in response to new information becomes more difficult as training progresses. While many recent works provide analyses and partial solutions to this phenomenon, a fundamental question remains unanswered: to what extent do known mechanisms of plasticity loss overlap, and how can mitigation strategies be combined to best maintain the trainability of a network? This paper addresses these questions, showing that loss of plasticity can be decomposed into multiple independent mechanisms and that, while intervening on any single mechanism is insufficient to avoid the loss of plasticity in all cases, intervening on multiple mechanisms in conjunction results in highly robust learning algorithms. We show that a combination of layer normalization and weight decay is highly effective at maintaining plasticity in a variety of synthetic nonstationary learning tasks, and further demonstrate its effectiveness on naturally arising nonstationarities, including reinforcement learning in the Arcade Learning Environment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/BSELJ64U/Lyle et al. - 2024 - Disentangling the Causes of Plasticity Loss in Neu.pdf;/Users/darshanpatil/Zotero/storage/9C4EHETJ/2402.html}
}

@misc{lyleDisentanglingCausesPlasticity2024a,
  title = {Disentangling the {{Causes}} of {{Plasticity Loss}} in {{Neural Networks}}},
  author = {Lyle, Clare and Zheng, Zeyu and Khetarpal, Khimya and {van Hasselt}, Hado and Pascanu, Razvan and Martens, James and Dabney, Will},
  year = {2024},
  month = feb,
  number = {arXiv:2402.18762},
  eprint = {2402.18762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.18762},
  urldate = {2024-04-09},
  abstract = {Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly innocuous assumption: that the network is trained on a {\textbackslash}textit\{stationary\} data distribution. In settings where this assumption is violated, e.g.{\textbackslash} deep reinforcement learning, learning algorithms become unstable and brittle with respect to hyperparameters and even random seeds. One factor driving this instability is the loss of plasticity, meaning that updating the network's predictions in response to new information becomes more difficult as training progresses. While many recent works provide analyses and partial solutions to this phenomenon, a fundamental question remains unanswered: to what extent do known mechanisms of plasticity loss overlap, and how can mitigation strategies be combined to best maintain the trainability of a network? This paper addresses these questions, showing that loss of plasticity can be decomposed into multiple independent mechanisms and that, while intervening on any single mechanism is insufficient to avoid the loss of plasticity in all cases, intervening on multiple mechanisms in conjunction results in highly robust learning algorithms. We show that a combination of layer normalization and weight decay is highly effective at maintaining plasticity in a variety of synthetic nonstationary learning tasks, and further demonstrate its effectiveness on naturally arising nonstationarities, including reinforcement learning in the Arcade Learning Environment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/GKGJAH5N/Lyle et al. - 2024 - Disentangling the Causes of Plasticity Loss in Neu.pdf;/Users/darshanpatil/Zotero/storage/E2IPVPWK/2402.html}
}

@misc{lyleLearningDynamicsGeneralization2022,
  title = {Learning {{Dynamics}} and {{Generalization}} in {{Reinforcement Learning}}},
  author = {Lyle, Clare and Rowland, Mark and Dabney, Will and Kwiatkowska, Marta and Gal, Yarin},
  year = {2022},
  month = jun,
  number = {arXiv:2206.02126},
  eprint = {2206.02126},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-27},
  abstract = {Solving a reinforcement learning (RL) problem poses two competing challenges: fitting a potentially discontinuous value function, and generalizing well to new observations. In this paper, we analyze the learning dynamics of temporal difference algorithms to gain novel insight into the tension between these two objectives. We show theoretically that temporal difference learning encourages agents to fit non-smooth components of the value function early in training, and at the same time induces the second-order effect of discouraging generalization. We corroborate these findings in deep RL agents trained on a range of environments, finding that neural networks trained using temporal difference algorithms on dense reward tasks exhibit weaker generalization between states than randomly initialized networks and networks trained with policy gradient methods. Finally, we investigate how post-training policy distillation may avoid this pitfall, and show that this approach improves generalization to novel environments in the ProcGen suite and improves robustness to input perturbations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,plasticity},
  file = {/Users/darshanpatil/Zotero/storage/W3756CCC/Lyle et al. - 2022 - Learning Dynamics and Generalization in Reinforcem.pdf;/Users/darshanpatil/Zotero/storage/CKD9FWIQ/2206.html}
}

@misc{lyleUnderstandingPlasticityNeural2023,
  title = {Understanding Plasticity in Neural Networks},
  author = {Lyle, Clare and Zheng, Zeyu and Nikishin, Evgenii and Pires, Bernardo Avila and Pascanu, Razvan and Dabney, Will},
  year = {2023},
  month = may,
  number = {arXiv:2303.01486},
  eprint = {2303.01486},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-27},
  abstract = {Plasticity, the ability of a neural network to quickly change its predictions in response to new information, is essential for the adaptability and robustness of deep reinforcement learning systems. Deep neural networks are known to lose plasticity over the course of training even in relatively simple learning problems, but the mechanisms driving this phenomenon are still poorly understood. This paper conducts a systematic empirical analysis into plasticity loss, with the goal of understanding the phenomenon mechanistically in order to guide the future development of targeted solutions. We find that loss of plasticity is deeply connected to changes in the curvature of the loss landscape, but that it typically occurs in the absence of saturated units or divergent gradient norms. Based on this insight, we identify a number of parameterization and optimization design choices which enable networks to better preserve plasticity over the course of training. We validate the utility of these findings in larger-scale learning problems by applying the best-performing intervention, layer normalization, to a deep RL agent trained on the Arcade Learning Environment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,plasticity},
  file = {/Users/darshanpatil/Zotero/storage/BAQULZF3/Lyle et al. - 2023 - Understanding plasticity in neural networks.pdf;/Users/darshanpatil/Zotero/storage/R274EGJK/2303.html}
}

@misc{lyleUnderstandingPreventingCapacity2022,
  title = {Understanding and {{Preventing Capacity Loss}} in {{Reinforcement Learning}}},
  author = {Lyle, Clare and Rowland, Mark and Dabney, Will},
  year = {2022},
  month = may,
  number = {arXiv:2204.09560},
  eprint = {2204.09560},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-27},
  abstract = {The reinforcement learning (RL) problem is rife with sources of non-stationarity, making it a notoriously difficult problem domain for the application of neural networks. We identify a mechanism by which non-stationary prediction targets can prevent learning progress in deep RL agents: {\textbackslash}textit\{capacity loss\}, whereby networks trained on a sequence of target values lose their ability to quickly update their predictions over time. We demonstrate that capacity loss occurs in a range of RL agents and environments, and is particularly damaging to performance in sparse-reward tasks. We then present a simple regularizer, Initial Feature Regularization (InFeR), that mitigates this phenomenon by regressing a subspace of features towards its value at initialization, leading to significant performance improvements in sparse-reward environments such as Montezuma's Revenge. We conclude that preventing capacity loss is crucial to enable agents to maximally benefit from the learning signals they obtain throughout the entire training trajectory.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,plasticity},
  file = {/Users/darshanpatil/Zotero/storage/E93L4PVE/Lyle et al. - 2022 - Understanding and Preventing Capacity Loss in Rein.pdf;/Users/darshanpatil/Zotero/storage/IK4CW554/2204.html}
}

@misc{maRevisitingPlasticityVisual2023,
  title = {Revisiting {{Plasticity}} in {{Visual Reinforcement Learning}}: {{Data}}, {{Modules}} and {{Training Stages}}},
  shorttitle = {Revisiting {{Plasticity}} in {{Visual Reinforcement Learning}}},
  author = {Ma, Guozheng and Li, Lu and Zhang, Sen and Liu, Zixuan and Wang, Zhen and Chen, Yixin and Shen, Li and Wang, Xueqian and Tao, Dacheng},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07418},
  eprint = {2310.07418},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-05},
  abstract = {Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased reuse frequency. Rather than setting a static RR for the entire training process, we propose Adaptive RR, which dynamically adjusts the RR based on the critic's plasticity level. Extensive evaluations indicate that Adaptive RR not only avoids catastrophic plasticity loss in the early stages but also benefits from more frequent reuse in later phases, resulting in superior sample efficiency.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/CLX9ZRFX/Ma et al. - 2023 - Revisiting Plasticity in Visual Reinforcement Lear.pdf;/Users/darshanpatil/Zotero/storage/YWMJFW3N/2310.html}
}

@misc{mirzadehLinearModeConnectivity2020,
  title = {Linear {{Mode Connectivity}} in {{Multitask}} and {{Continual Learning}}},
  author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Gorur, Dilan and Pascanu, Razvan and Ghasemzadeh, Hassan},
  year = {2020},
  month = oct,
  number = {arXiv:2010.04495},
  eprint = {2010.04495},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.04495},
  urldate = {2023-06-27},
  abstract = {Continual (sequential) training and multitask (simultaneous) training are often attempting to solve the same overall objective: to find a solution that performs well on all considered tasks. The main difference is in the training regimes, where continual learning can only have access to one task at a time, which for neural networks typically leads to catastrophic forgetting. That is, the solution found for a subsequent task does not perform well on the previous ones anymore. However, the relationship between the different minima that the two training regimes arrive at is not well understood. What sets them apart? Is there a local structure that could explain the difference in performance achieved by the two different schemes? Motivated by recent work showing that different minima of the same task are typically connected by very simple curves of low error, we investigate whether multitask and continual solutions are similarly connected. We empirically find that indeed such connectivity can be reliably achieved and, more interestingly, it can be done by a linear path, conditioned on having the same initialization for both. We thoroughly analyze this observation and discuss its significance for the continual learning process. Furthermore, we exploit this finding to propose an effective algorithm that constrains the sequentially learned minima to behave as the multitask solution. We show that our method outperforms several state of the art continual learning algorithms on various vision benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,plasticity},
  file = {/Users/darshanpatil/Zotero/storage/73JG2WJ4/Mirzadeh et al. - 2020 - Linear Mode Connectivity in Multitask and Continua.pdf;/Users/darshanpatil/Zotero/storage/TZ5YA8V3/2010.html}
}

@misc{morcosOneTicketWin2019,
  title = {One Ticket to Win Them All: Generalizing Lottery Ticket Initializations across Datasets and Optimizers},
  shorttitle = {One Ticket to Win Them All},
  author = {Morcos, Ari S. and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
  year = {2019},
  month = oct,
  number = {arXiv:1906.02773},
  eprint = {1906.02773},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-07-22},
  abstract = {The success of lottery ticket initializations (Frankle and Carbin, 2019) suggests that small, sparsified networks can be trained so long as the network is initialized appropriately. Unfortunately, finding these "winning ticket" initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training configuration (optimizer and dataset) and evaluating their performance on another configuration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations generated by sufficiently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,plasticity,Statistics - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/9DUZHAMF/Morcos et al. - 2019 - One ticket to win them all generalizing lottery t.pdf;/Users/darshanpatil/Zotero/storage/VTPDCMKM/1906.html}
}

@inproceedings{nikishinDeepReinforcementLearning2023,
  title = {Deep {{Reinforcement Learning}} with {{Plasticity Injection}}},
  booktitle = {Workshop on {{Reincarnating Reinforcement Learning}} at {{ICLR}} 2023},
  author = {Nikishin, Evgenii and Oh, Junhyuk and Ostrovski, Georg and Lyle, Clare and Pascanu, Razvan and Dabney, Will and Barreto, Andre},
  year = {2023},
  month = mar,
  urldate = {2023-06-16},
  abstract = {A growing body of evidence suggests that neural networks employed in deep reinforcement learning (RL) gradually lose their plasticity, the ability to learn from new data; however, the analysis and mitigation of this phenomenon is hampered by the complex relationship between plasticity, exploration, and performance in RL. This paper introduces plasticity injection, a minimalistic intervention that increases the network plasticity without changing the number of trainable parameters or biasing the predictions. The applications of this intervention are two-fold: first, as a diagnostic tool --- if injection increases the performance, we may conclude that an agent's network was losing its plasticity. This tool allows us to identify a subset of Atari environments where the lack of plasticity causes performance plateaus, motivating future studies on understanding and combating plasticity loss. Second, plasticity injection can be used to improve the computational efficiency of RL training if the agent has to re-learn from scratch due to exhausted plasticity or by growing the agent's network dynamically without compromising performance. The results on Atari show that plasticity injection attains stronger performance compared to alternative methods while being computationally efficient.},
  langid = {english},
  keywords = {plasticity},
  file = {/Users/darshanpatil/Zotero/storage/762IKLX8/Nikishin et al. - 2023 - Deep Reinforcement Learning with Plasticity Inject.pdf}
}

@misc{nikishinDeepReinforcementLearning2023a,
  title = {Deep {{Reinforcement Learning}} with {{Plasticity Injection}}},
  author = {Nikishin, Evgenii and Oh, Junhyuk and Ostrovski, Georg and Lyle, Clare and Pascanu, Razvan and Dabney, Will and Barreto, Andr{\'e}},
  year = {2023},
  month = oct,
  number = {arXiv:2305.15555},
  eprint = {2305.15555},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-24},
  abstract = {A growing body of evidence suggests that neural networks employed in deep reinforcement learning (RL) gradually lose their plasticity, the ability to learn from new data; however, the analysis and mitigation of this phenomenon is hampered by the complex relationship between plasticity, exploration, and performance in RL. This paper introduces plasticity injection, a minimalistic intervention that increases the network plasticity without changing the number of trainable parameters or biasing the predictions. The applications of this intervention are two-fold: first, as a diagnostic tool \${\textbackslash}unicode\{x2014\}\$ if injection increases the performance, we may conclude that an agent's network was losing its plasticity. This tool allows us to identify a subset of Atari environments where the lack of plasticity causes performance plateaus, motivating future studies on understanding and combating plasticity loss. Second, plasticity injection can be used to improve the computational efficiency of RL training if the agent has to re-learn from scratch due to exhausted plasticity or by growing the agent's network dynamically without compromising performance. The results on Atari show that plasticity injection attains stronger performance compared to alternative methods while being computationally efficient.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/GWF9G8PF/Nikishin et al. - 2023 - Deep Reinforcement Learning with Plasticity Inject.pdf;/Users/darshanpatil/Zotero/storage/VGV2FGD5/2305.html}
}

@inproceedings{nikishinPrimacyBiasDeep2022,
  title = {The {{Primacy Bias}} in {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Nikishin, Evgenii and Schwarzer, Max and D'Oro, Pierluca and Bacon, Pierre-Luc and Courville, Aaron},
  year = {2022},
  month = jun,
  pages = {16828--16847},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-06-19},
  abstract = {This work identifies a common flaw of deep reinforcement learning (RL) algorithms: a tendency to rely on early interactions and ignore useful evidence encountered later. Because of training on progressively growing datasets, deep RL agents incur a risk of overfitting to earlier experiences, negatively affecting the rest of the learning process. Inspired by cognitive science, we refer to this effect as the primacy bias. Through a series of experiments, we dissect the algorithmic aspects of deep RL that exacerbate this bias. We then propose a simple yet generally-applicable mechanism that tackles the primacy bias by periodically resetting a part of the agent. We apply this mechanism to algorithms in both discrete (Atari 100k) and continuous action (DeepMind Control Suite) domains, consistently improving their performance.},
  langid = {english},
  keywords = {plasticity},
  file = {/Users/darshanpatil/Zotero/storage/RSQJSUPG/Nikishin et al. - 2022 - The Primacy Bias in Deep Reinforcement Learning.pdf}
}

@article{pushkinMultilayerLookaheadNested2021,
  title = {Multilayer {{Lookahead}}: A {{Nested Version}} of {{Lookahead}}},
  shorttitle = {Multilayer {{Lookahead}}},
  author = {Pushkin, Denys and Barba, Luis},
  year = {2021},
  journal = {ArXiv},
  urldate = {2023-07-25},
  abstract = {An academic search engine that utilizes artificial intelligence methods to provide highly relevant results and novel tools to filter them with ease.},
  langid = {english},
  keywords = {plasticity},
  file = {/Users/darshanpatil/Zotero/storage/WP6SBXDZ/Pushkin and Barba - 2021 - Multilayer Lookahead a Nested Version of Lookahea.pdf}
}

@article{richardsDendriticSolutionsCredit2019,
  title = {Dendritic Solutions to the Credit Assignment Problem},
  author = {Richards, Blake A and Lillicrap, Timothy P},
  year = {2019},
  month = feb,
  journal = {Current Opinion in Neurobiology},
  series = {Neurobiology of {{Learning}} and {{Plasticity}}},
  volume = {54},
  pages = {28--36},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2018.08.003},
  urldate = {2021-10-07},
  abstract = {Guaranteeing that synaptic plasticity leads to effective learning requires a means for assigning credit to each neuron for its contribution to behavior. The `credit assignment problem' refers to the fact that credit assignment is non-trivial in hierarchical networks with multiple stages of processing. One difficulty is that if credit signals are integrated with other inputs, then it is hard for synaptic plasticity rules to distinguish credit-related activity from non-credit-related activity. A potential solution is to use the spatial layout and non-linear properties of dendrites to distinguish credit signals from other inputs. In cortical pyramidal neurons, evidence hints that top-down feedback signals are integrated in the distal apical dendrites and have a distinct impact on spike-firing and synaptic plasticity. This suggests that the distal apical dendrites of pyramidal neurons help the brain to solve the credit assignment problem.},
  langid = {english}
}

@misc{sadrtdinovStayNotStay2023,
  title = {To {{Stay}} or {{Not}} to {{Stay}} in the {{Pre-train Basin}}: {{Insights}} on {{Ensembling}} in {{Transfer Learning}}},
  shorttitle = {To {{Stay}} or {{Not}} to {{Stay}} in the {{Pre-train Basin}}},
  author = {Sadrtdinov, Ildus and Pozdeev, Dmitrii and Vetrov, Dmitry and Lobacheva, Ekaterina},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03374},
  eprint = {2303.03374},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-07-04},
  abstract = {Transfer learning and ensembling are two popular techniques for improving the performance and robustness of neural networks. Due to the high cost of pre-training, ensembles of models fine-tuned from a single pre-trained checkpoint are often used in practice. Such models end up in the same basin of the loss landscape and thus have limited diversity. In this work, we study if it is possible to improve ensembles trained from a single pre-trained checkpoint by better exploring the pre-train basin or a close vicinity outside of it. We show that while exploration of the pre-train basin may be beneficial for the ensemble, leaving the basin results in losing the benefits of transfer learning and degradation of the ensemble quality.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,plasticity,Statistics - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/IU2QUR3D/Sadrtdinov et al. - 2023 - To Stay or Not to Stay in the Pre-train Basin Ins.pdf;/Users/darshanpatil/Zotero/storage/UBWAJ3GM/2303.html}
}

@misc{sokarDormantNeuronPhenomenon2023,
  title = {The {{Dormant Neuron Phenomenon}} in {{Deep Reinforcement Learning}}},
  author = {Sokar, Ghada and Agarwal, Rishabh and Castro, Pablo Samuel and Evci, Utku},
  year = {2023},
  month = jun,
  number = {arXiv:2302.12902},
  eprint = {2302.12902},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-16},
  abstract = {In this work we identify the dormant neuron phenomenon in deep reinforcement learning, where an agent's network suffers from an increasing number of inactive neurons, thereby affecting network expressivity. We demonstrate the presence of this phenomenon across a variety of algorithms and environments, and highlight its effect on learning. To address this issue, we propose a simple and effective method (ReDo) that Recycles Dormant neurons throughout training. Our experiments demonstrate that ReDo maintains the expressive power of networks by reducing the number of dormant neurons and results in improved performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,plasticity},
  file = {/Users/darshanpatil/Zotero/storage/W29XTGD8/Sokar et al. - 2023 - The Dormant Neuron Phenomenon in Deep Reinforcemen.pdf;/Users/darshanpatil/Zotero/storage/TTPYXNYW/2302.html}
}

@misc{vyasImplicitBiasInsignificance2023,
  title = {Beyond {{Implicit Bias}}: {{The Insignificance}} of {{SGD Noise}} in {{Online Learning}}},
  shorttitle = {Beyond {{Implicit Bias}}},
  author = {Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Kaplun, Gal and Kakade, Sham and Barak, Boaz},
  year = {2023},
  month = jun,
  number = {arXiv:2306.08590},
  eprint = {2306.08590},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2306.08590},
  urldate = {2023-06-21},
  abstract = {The success of SGD in deep learning has been ascribed by prior works to the implicit bias induced by high learning rate or small batch size ("SGD noise"). While prior works that focused on offline learning (i.e., multiple-epoch training), we study the impact of SGD noise on online (i.e., single epoch) learning. Through an extensive empirical analysis of image and language data, we demonstrate that large learning rate and small batch size do not confer any implicit bias advantages in online learning. In contrast to offline learning, the benefits of SGD noise in online learning are strictly computational, facilitating larger or more cost-effective gradient steps. Our work suggests that SGD in the online regime can be construed as taking noisy steps along the "golden path" of the noiseless gradient flow algorithm. We provide evidence to support this hypothesis by conducting experiments that reduce SGD noise during training and by measuring the pointwise functional distance between models trained with varying SGD noise levels, but at equivalent loss values. Our findings challenge the prevailing understanding of SGD and offer novel insights into its role in online learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,plasticity,Statistics - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/W7YQSDC2/Vyas et al. - 2023 - Beyond Implicit Bias The Insignificance of SGD Noise in Online Learning.pdf}
}

@misc{zhangLookaheadOptimizerSteps2019,
  title = {Lookahead {{Optimizer}}: k Steps Forward, 1 Step Back},
  shorttitle = {Lookahead {{Optimizer}}},
  author = {Zhang, Michael R. and Lucas, James and Hinton, Geoffrey and Ba, Jimmy},
  year = {2019},
  month = jul,
  number = {arXiv:1907.08610},
  eprint = {1907.08610},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-07-25},
  abstract = {The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of fast weights generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,plasticity,Statistics - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/8DMK3IGU/Zhang et al. - 2019 - Lookahead Optimizer k steps forward, 1 step back.pdf;/Users/darshanpatil/Zotero/storage/ZRARUKUB/Zhang et al. - 2019 - Lookahead Optimizer k steps forward, 1 step back.pdf;/Users/darshanpatil/Zotero/storage/M5DYVZMZ/1907.html}
}

@misc{ziyinSymmetryLeadsStructured2023,
  title = {Symmetry {{Leads}} to {{Structured Constraint}} of {{Learning}}},
  author = {Ziyin, Liu},
  year = {2023},
  month = sep,
  number = {arXiv:2309.16932},
  eprint = {2309.16932},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-01-24},
  abstract = {Due to common architecture designs, symmetries exist extensively in contemporary neural networks. In this work, we unveil the importance of the loss function symmetries in affecting, if not deciding, the learning behavior of machine learning models. We prove that every mirror symmetry of the loss function leads to a structured constraint, which becomes a favored solution when either the weight decay or gradient noise is large. As direct corollaries, we show that rescaling symmetry leads to sparsity, rotation symmetry leads to low rankness, and permutation symmetry leads to homogeneous ensembling. Then, we show that the theoretical framework can explain the loss of plasticity and various collapse phenomena in neural networks and suggest how symmetries can be used to design algorithms to enforce hard constraints in a differentiable way.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/C7SD965X/Ziyin - 2023 - Symmetry Leads to Structured Constraint of Learnin.pdf;/Users/darshanpatil/Zotero/storage/8MJIB2TR/2309.html}
}

@misc{ziyinSymmetryLeadsStructured2023a,
  title = {Symmetry {{Leads}} to {{Structured Constraint}} of {{Learning}}},
  author = {Ziyin, Liu},
  year = {2023},
  month = sep,
  number = {arXiv:2309.16932},
  eprint = {2309.16932},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.16932},
  urldate = {2024-05-08},
  abstract = {Due to common architecture designs, symmetries exist extensively in contemporary neural networks. In this work, we unveil the importance of the loss function symmetries in affecting, if not deciding, the learning behavior of machine learning models. We prove that every mirror symmetry of the loss function leads to a structured constraint, which becomes a favored solution when either the weight decay or gradient noise is large. As direct corollaries, we show that rescaling symmetry leads to sparsity, rotation symmetry leads to low rankness, and permutation symmetry leads to homogeneous ensembling. Then, we show that the theoretical framework can explain the loss of plasticity and various collapse phenomena in neural networks and suggest how symmetries can be used to design algorithms to enforce hard constraints in a differentiable way.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/darshanpatil/Zotero/storage/9GVLIYH6/Ziyin - 2023 - Symmetry Leads to Structured Constraint of Learnin.pdf;/Users/darshanpatil/Zotero/storage/AEKR7U63/2309.html}
}
